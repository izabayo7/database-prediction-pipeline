{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Employee Income Prediction Script\n",
        "\n",
        "This notebook fetches the latest employee entry from the API, preprocesses the data, makes predictions using the trained linear regression model, and logs the results to the database.\n",
        "\n",
        "## Steps:\n",
        "1. Import required libraries and setup\n",
        "2. Fetch the latest employee entry from the API\n",
        "3. Handle missing data and preprocess the input\n",
        "4. Load the trained model and scaler\n",
        "5. Make predictions\n",
        "6. Log results to the database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/patrickniyo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path to import from task_2_api\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'task_2_api'))\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# API Configuration\n",
        "API_BASE_URL = \"http://localhost:8000\"  # Update if your API runs on a different port\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Fetch Latest Employee Entry from API\n",
        "\n",
        "Fetches the last entered employee record based on creation timestamp (`created_at`), not employee_number.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest employee found: Employee #2069\n",
            "Created at: N/A\n",
            "\n",
            "Employee data:\n",
            "{\n",
            "  \"age\": 21,\n",
            "  \"gender\": \"Female\",\n",
            "  \"marital_status\": \"Single\",\n",
            "  \"education\": 2,\n",
            "  \"education_field\": \"Life Sciences\",\n",
            "  \"distance_from_home\": 1,\n",
            "  \"over_18\": \"Y\",\n",
            "  \"employee_count\": 1,\n",
            "  \"attrition\": \"Yes\",\n",
            "  \"department_name\": null,\n",
            "  \"job_satisfaction\": 3,\n",
            "  \"employee_number\": 2069\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_latest_employee():\n",
        "    \"\"\"\n",
        "    Fetch the latest employee entry from the API.\n",
        "    Returns the employee with the most recent created_at timestamp (last entered record).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch the latest employee by creation timestamp\n",
        "        response = requests.get(f\"{API_BASE_URL}/mysql/employees/latest/entry\")\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        latest_employee = response.json()\n",
        "        \n",
        "        if not latest_employee:\n",
        "            raise ValueError(\"No employees found in the database\")\n",
        "        \n",
        "        employee_number = latest_employee.get('employee_number')\n",
        "        created_at = latest_employee.get('created_at', 'N/A')\n",
        "        \n",
        "        print(f\"Latest employee found: Employee #{employee_number}\")\n",
        "        print(f\"Created at: {created_at}\")\n",
        "        return latest_employee\n",
        "    \n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(f\"ERROR: Could not connect to API at {API_BASE_URL}\")\n",
        "        print(\"Please make sure the API server is running:\")\n",
        "        print(\"  cd task_2_api\")\n",
        "        print(\"  uvicorn main:app --reload\")\n",
        "        return None\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            print(\"ERROR: No employees found in the database\")\n",
        "            print(\"Please create at least one employee entry first\")\n",
        "        else:\n",
        "            print(f\"HTTP Error: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching employee data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fetch the latest employee\n",
        "latest_employee = get_latest_employee()\n",
        "if latest_employee:\n",
        "    print(f\"\\nEmployee data:\")\n",
        "    print(json.dumps(latest_employee, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fetch Additional Employee Details (Job Details, etc.)\n",
        "\n",
        "Since the employee entry might not have all features needed for prediction, we need to fetch related data or construct it from available fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: The API employee structure might differ from the CSV format.\n",
            "We'll map available fields and use default values for missing ones.\n"
          ]
        }
      ],
      "source": [
        "# For prediction, we need to map the API employee data to the format expected by our model\n",
        "# Since the API might not return all fields from the CSV, we'll need to handle missing fields\n",
        "\n",
        "print(\"Note: The API employee structure might differ from the CSV format.\")\n",
        "print(\"We'll map available fields and use default values for missing ones.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Trained Model and Preprocessing Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and preprocessing tools loaded successfully!\n",
            "Model expects 30 features\n",
            "Features: ['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "model_path = 'models/employee_income_model.joblib'\n",
        "scaler_path = 'models/employee_income_scaler.joblib'\n",
        "feature_names_path = 'models/feature_names.json'\n",
        "label_encoders_path = 'models/label_encoders.joblib'\n",
        "\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    \n",
        "    with open(feature_names_path, 'r') as f:\n",
        "        feature_names = json.load(f)\n",
        "    \n",
        "    label_encoders = joblib.load(label_encoders_path)\n",
        "    \n",
        "    print(\"Model and preprocessing tools loaded successfully!\")\n",
        "    print(f\"Model expects {len(feature_names)} features\")\n",
        "    print(f\"Features: {feature_names}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Model files not found. Please run the training notebook first!\")\n",
        "    print(f\"Missing file: {e.filename}\")\n",
        "    model = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Preprocess Employee Data for Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessed feature data:\n",
            "   Age  Attrition  BusinessTravel  DailyRate  Department  DistanceFromHome  \\\n",
            "0   21          0               2      802.0           1                 1   \n",
            "\n",
            "   Education  EducationField  EnvironmentSatisfaction  Gender  ...  \\\n",
            "0          2               1                      3.0       0  ...   \n",
            "\n",
            "   PerformanceRating  RelationshipSatisfaction  StockOptionLevel  \\\n",
            "0                3.0                       3.0               1.0   \n",
            "\n",
            "   TotalWorkingYears  TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  \\\n",
            "0               10.0                    3.0              3.0             5.0   \n",
            "\n",
            "   YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
            "0                 3.0                      1.0                   3.0  \n",
            "\n",
            "[1 rows x 30 columns]\n",
            "\n",
            "Raw processed data sample:\n",
            "  Age: 21\n",
            "  Attrition: No\n",
            "  BusinessTravel: Travel_Rarely\n",
            "  DailyRate: 802.0\n",
            "  Department: Research & Development\n",
            "  DistanceFromHome: 1\n",
            "  Education: 2\n",
            "  EducationField: Life Sciences\n",
            "  EnvironmentSatisfaction: 3.0\n",
            "  Gender: Female\n"
          ]
        }
      ],
      "source": [
        "def preprocess_employee_data(employee_data, feature_names, label_encoders):\n",
        "    \"\"\"\n",
        "    Preprocess employee data to match the model's expected format.\n",
        "    Handles missing data and encodes categorical variables.\n",
        "    \n",
        "    Maps API fields (lowercase/snake_case) to CSV column names (TitleCase).\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default values from the original dataset\n",
        "    # Load original CSV to get defaults and understand data ranges\n",
        "    try:\n",
        "        original_data = pd.read_csv('../hr_employee_attrition.csv')\n",
        "        default_values = {}\n",
        "        \n",
        "        # Calculate median/mode for each feature\n",
        "        for col in feature_names:\n",
        "            if col in original_data.columns:\n",
        "                if original_data[col].dtype in ['int64', 'float64']:\n",
        "                    default_values[col] = float(original_data[col].median())\n",
        "                else:\n",
        "                    mode_val = original_data[col].mode()\n",
        "                    default_values[col] = mode_val[0] if len(mode_val) > 0 else 0\n",
        "            else:\n",
        "                # Use reasonable defaults\n",
        "                default_values[col] = 0\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load original CSV for defaults: {e}\")\n",
        "        default_values = {col: 0 for col in feature_names}\n",
        "    \n",
        "    # Map API employee data to feature format\n",
        "    # API returns fields in lowercase/snake_case, CSV uses TitleCase\n",
        "    processed_data = {}\n",
        "    \n",
        "    # Mapping from API field names (lowercase/snake_case) to CSV column names (TitleCase)\n",
        "    api_to_csv_mapping = {\n",
        "        'age': 'Age',\n",
        "        'gender': 'Gender',\n",
        "        'marital_status': 'MaritalStatus',\n",
        "        'education': 'Education',\n",
        "        'education_field': 'EducationField',\n",
        "        'distance_from_home': 'DistanceFromHome',\n",
        "        'department_name': 'Department',\n",
        "        'business_travel': 'BusinessTravel',\n",
        "        'over_time': 'OverTime',\n",
        "    }\n",
        "    \n",
        "    # Start with default values for all features\n",
        "    for feature in feature_names:\n",
        "        processed_data[feature] = default_values.get(feature, 0)\n",
        "    \n",
        "    # Map available employee data from API\n",
        "    employee_dict = employee_data if isinstance(employee_data, dict) else employee_data.__dict__\n",
        "    \n",
        "    # Map fields from API format to CSV format\n",
        "    for api_field, csv_field in api_to_csv_mapping.items():\n",
        "        if api_field in employee_dict and csv_field in feature_names:\n",
        "            value = employee_dict[api_field]\n",
        "            # Handle None values\n",
        "            if value is None:\n",
        "                # Use default value instead\n",
        "                if csv_field in default_values:\n",
        "                    processed_data[csv_field] = default_values[csv_field]\n",
        "                else:\n",
        "                    processed_data[csv_field] = 0  # Default fallback\n",
        "            else:\n",
        "                processed_data[csv_field] = value\n",
        "    \n",
        "    # Handle nested job_details if available\n",
        "    if 'job_details' in employee_dict and employee_dict['job_details']:\n",
        "        job_details = employee_dict['job_details']\n",
        "        # Map job-related fields\n",
        "        if 'job_level' in job_details and 'JobLevel' in feature_names:\n",
        "            processed_data['JobLevel'] = job_details['job_level']\n",
        "        if 'job_involvement' in job_details and 'JobInvolvement' in feature_names:\n",
        "            processed_data['JobInvolvement'] = job_details['job_involvement']\n",
        "        if 'job_satisfaction' in job_details and 'JobSatisfaction' in feature_names:\n",
        "            processed_data['JobSatisfaction'] = job_details['job_satisfaction']\n",
        "        if 'business_travel' in job_details and 'BusinessTravel' in feature_names:\n",
        "            processed_data['BusinessTravel'] = job_details['business_travel']\n",
        "        if 'overtime' in job_details and 'OverTime' in feature_names:\n",
        "            processed_data['OverTime'] = job_details['overtime']\n",
        "    \n",
        "    # Replace None values with defaults before creating DataFrame\n",
        "    for key, value in processed_data.items():\n",
        "        if value is None:\n",
        "            processed_data[key] = default_values.get(key, 0)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame([processed_data])\n",
        "    \n",
        "    # Encode categorical variables using saved label encoders\n",
        "    for col in df.columns:\n",
        "        if col in label_encoders:\n",
        "            try:\n",
        "                original_val = df[col].iloc[0]\n",
        "                # Only encode if it's a string\n",
        "                if isinstance(original_val, str):\n",
        "                    # Check if the value is in the encoder's classes\n",
        "                    if original_val in label_encoders[col].classes_:\n",
        "                        df[col] = label_encoders[col].transform([original_val])[0]\n",
        "                    else:\n",
        "                        print(f\"Warning: '{original_val}' not in label encoder for {col}, using default\")\n",
        "                        # Use the first encoded value as default\n",
        "                        if len(label_encoders[col].classes_) > 0:\n",
        "                            df[col] = label_encoders[col].transform([label_encoders[col].classes_[0]])[0]\n",
        "                        else:\n",
        "                            df[col] = 0\n",
        "                # If it's numeric, assume it's already encoded\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode {col}: {e}\")\n",
        "                # Use default from original dataset\n",
        "                if col in default_values:\n",
        "                    default_val = default_values[col]\n",
        "                    if isinstance(default_val, str) and col in label_encoders:\n",
        "                        try:\n",
        "                            df[col] = label_encoders[col].transform([default_val])[0]\n",
        "                        except:\n",
        "                            df[col] = 0\n",
        "                    else:\n",
        "                        df[col] = default_val if isinstance(default_val, (int, float)) else 0\n",
        "                else:\n",
        "                    df[col] = 0\n",
        "    \n",
        "    # Ensure all feature columns are present and in correct order\n",
        "    for feature in feature_names:\n",
        "        if feature not in df.columns:\n",
        "            df[feature] = default_values.get(feature, 0)\n",
        "        # Ensure numeric types are correct\n",
        "        if feature in default_values and isinstance(default_values[feature], (int, float)):\n",
        "            df[feature] = pd.to_numeric(df[feature], errors='coerce').fillna(default_values[feature])\n",
        "    \n",
        "    # Select features in the correct order\n",
        "    feature_df = df[feature_names]\n",
        "    \n",
        "    # Final check: Fill any remaining NaN/None values with defaults\n",
        "    for col in feature_df.columns:\n",
        "        # Check for NaN or None values\n",
        "        if feature_df[col].isnull().any():\n",
        "            default_val = default_values.get(col, 0)\n",
        "            if isinstance(default_val, str):\n",
        "                # For string columns, try to encode to default or use 0\n",
        "                if col in label_encoders:\n",
        "                    try:\n",
        "                        default_val = label_encoders[col].transform([default_val])[0]\n",
        "                    except:\n",
        "                        default_val = 0\n",
        "                else:\n",
        "                    default_val = 0\n",
        "            feature_df[col] = feature_df[col].fillna(default_val)\n",
        "        \n",
        "        # Convert to numeric if it's not already, and fill NaN\n",
        "        try:\n",
        "            feature_df[col] = pd.to_numeric(feature_df[col], errors='coerce')\n",
        "            if feature_df[col].isnull().any():\n",
        "                feature_df[col] = feature_df[col].fillna(default_values.get(col, 0))\n",
        "        except:\n",
        "            # If conversion fails, use default\n",
        "            feature_df[col] = default_values.get(col, 0)\n",
        "    \n",
        "    # Final validation: Ensure no NaN values remain\n",
        "    if feature_df.isnull().any().any():\n",
        "        print(\"Warning: Found NaN values, filling with defaults...\")\n",
        "        for col in feature_df.columns:\n",
        "            if feature_df[col].isnull().any():\n",
        "                feature_df[col] = feature_df[col].fillna(default_values.get(col, 0))\n",
        "    \n",
        "    return feature_df, processed_data, default_values\n",
        "\n",
        "if latest_employee and model:\n",
        "    # Preprocess the employee data\n",
        "    feature_df, raw_data, default_values = preprocess_employee_data(latest_employee, feature_names, label_encoders)\n",
        "    \n",
        "    print(\"\\nPreprocessed feature data:\")\n",
        "    print(feature_df)\n",
        "    print(f\"\\nRaw processed data sample:\")\n",
        "    for key, value in list(raw_data.items())[:10]:\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Scale Features and Make Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating feature data...\n",
            "Feature data shape: (1, 30)\n",
            "NaN values in features: 0\n",
            "Data types: {dtype('float64'): 30}\n",
            "Scaled features shape: (1, 30)\n",
            "\n",
            "==================================================\n",
            "PREDICTION RESULT\n",
            "==================================================\n",
            "Employee Number: 2069\n",
            "Predicted Monthly Income: $6,836.28\n",
            "Prediction Date: 2025-10-31 22:36:07\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "if latest_employee and model:\n",
        "    # Validate that there are no NaN/None values before scaling\n",
        "    print(\"Validating feature data...\")\n",
        "    \n",
        "    # Ensure default_values is available (from preprocessing step)\n",
        "    # If not available, create defaults from the dataset\n",
        "    if 'default_values' not in globals() or default_values is None:\n",
        "        print(\"Warning: default_values not found, creating defaults...\")\n",
        "        try:\n",
        "            original_data = pd.read_csv('../hr_employee_attrition.csv')\n",
        "            default_values = {}\n",
        "            for col in feature_df.columns:\n",
        "                if col in original_data.columns:\n",
        "                    if original_data[col].dtype in ['int64', 'float64']:\n",
        "                        default_values[col] = float(original_data[col].median())\n",
        "                    else:\n",
        "                        mode_val = original_data[col].mode()\n",
        "                        default_values[col] = mode_val[0] if len(mode_val) > 0 else 0\n",
        "                else:\n",
        "                    default_values[col] = 0\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load defaults from CSV: {e}\")\n",
        "            default_values = {col: 0 for col in feature_df.columns}\n",
        "    \n",
        "    # Check for None values and replace\n",
        "    for col in feature_df.columns:\n",
        "        # Replace None with defaults\n",
        "        if feature_df[col].isnull().any():\n",
        "            default_val = default_values.get(col, 0)\n",
        "            # If default is string and we need numeric, use 0\n",
        "            if isinstance(default_val, str):\n",
        "                if col in label_encoders:\n",
        "                    try:\n",
        "                        default_val = label_encoders[col].transform([default_val])[0]\n",
        "                    except:\n",
        "                        default_val = 0\n",
        "                else:\n",
        "                    default_val = 0\n",
        "            feature_df[col] = feature_df[col].fillna(default_val)\n",
        "    \n",
        "    # Ensure all values are numeric (convert any remaining strings)\n",
        "    for col in feature_df.columns:\n",
        "        if feature_df[col].dtype == 'object':\n",
        "            # Try to convert to numeric\n",
        "            feature_df[col] = pd.to_numeric(feature_df[col], errors='coerce').fillna(0)\n",
        "        else:\n",
        "            feature_df[col] = pd.to_numeric(feature_df[col], errors='coerce').fillna(0)\n",
        "    \n",
        "    # Final check for NaN\n",
        "    nan_count = feature_df.isnull().sum().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Warning: {nan_count} NaN values still present, filling with 0...\")\n",
        "        feature_df = feature_df.fillna(0)\n",
        "    \n",
        "    # Ensure all columns are numeric type\n",
        "    feature_df = feature_df.astype(float)\n",
        "    \n",
        "    print(f\"Feature data shape: {feature_df.shape}\")\n",
        "    print(f\"NaN values in features: {feature_df.isnull().sum().sum()}\")\n",
        "    print(f\"Data types: {feature_df.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Scale the features\n",
        "    feature_scaled = scaler.transform(feature_df)\n",
        "    \n",
        "    print(f\"Scaled features shape: {feature_scaled.shape}\")\n",
        "    \n",
        "    # Check for NaN in scaled features\n",
        "    nan_in_scaled = np.isnan(feature_scaled).sum()\n",
        "    if nan_in_scaled > 0:\n",
        "        print(f\"Warning: {nan_in_scaled} NaN values found in scaled features, filling with 0...\")\n",
        "        feature_scaled = np.nan_to_num(feature_scaled, nan=0.0)\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(feature_scaled)[0]\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"PREDICTION RESULT\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Employee Number: {latest_employee.get('employee_number')}\")\n",
        "    print(f\"Predicted Monthly Income: ${prediction:,.2f}\")\n",
        "    print(f\"Prediction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'='*50}\")\n",
        "else:\n",
        "    print(\"Cannot make prediction - missing employee data or model\")\n",
        "    prediction = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Log Prediction Results to MongoDB via API\n",
        "\n",
        "We'll use the `/mongo/predictions/` API endpoint to store prediction results in MongoDB. This avoids foreign key constraints since MongoDB stores employee_number as a simple integer field without relations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Error connecting to database: Foreign key associated with column 'predictions.employee_number' could not find table 'employees' with which to generate a foreign key to target column 'employee_number'\n",
            "   Prediction will not be logged to database\n",
            "\n",
            "⚠️  Prediction made but could not be logged to database\n"
          ]
        }
      ],
      "source": [
        "def log_prediction_to_db(employee_number, predicted_income, raw_data, feature_data):\n",
        "    \"\"\"\n",
        "    Log prediction results to MongoDB via API endpoint.\n",
        "    Uses the /mongo/predictions/ endpoint which stores data in MongoDB\n",
        "    without foreign key constraints.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare prediction data\n",
        "        # Convert feature_data DataFrame to dictionary\n",
        "        if isinstance(feature_data, pd.DataFrame):\n",
        "            # Get the first (and only) row as a dictionary\n",
        "            input_features_dict = feature_data.iloc[0].to_dict()\n",
        "        else:\n",
        "            input_features_dict = feature_data.to_dict() if hasattr(feature_data, 'to_dict') else feature_data\n",
        "        \n",
        "        prediction_payload = {\n",
        "            \"employee_number\": employee_number,\n",
        "            \"predicted_monthly_income\": float(predicted_income),\n",
        "            \"input_features\": input_features_dict,\n",
        "            \"model_version\": \"v1.0\",\n",
        "            \"prediction_date\": datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        # Send POST request to API\n",
        "        response = requests.post(\n",
        "            f\"{API_BASE_URL}/mongo/predictions/\",\n",
        "            json=prediction_payload,\n",
        "            headers={\"Content-Type\": \"application/json\"}\n",
        "        )\n",
        "        \n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = response.json()\n",
        "        \n",
        "        print(f\"\\n✅ Prediction logged to MongoDB successfully!\")\n",
        "        print(f\"   Prediction ID: {result.get('_id', result.get('prediction_id', 'N/A'))}\")\n",
        "        print(f\"   Employee Number: {employee_number}\")\n",
        "        print(f\"   Predicted Income: ${predicted_income:,.2f}\")\n",
        "        print(f\"   Model Version: {result.get('model_version', 'v1.0')}\")\n",
        "        print(f\"   Logged at: {result.get('prediction_date', 'N/A')}\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(f\"❌ Error: Could not connect to API at {API_BASE_URL}\")\n",
        "        print(\"   Please make sure the API server is running:\")\n",
        "        print(\"   cd task_2_api\")\n",
        "        print(\"   uvicorn main:app --reload\")\n",
        "        return None\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"❌ HTTP Error logging prediction: {e}\")\n",
        "        if e.response.status_code == 404:\n",
        "            print(\"   Endpoint not found. Please check the API is running and includes the predictions router.\")\n",
        "        else:\n",
        "            print(f\"   Response: {e.response.text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error logging prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "if latest_employee and model and prediction is not None:\n",
        "    # Log prediction to database via API\n",
        "    employee_num = latest_employee.get('employee_number')\n",
        "    result = log_prediction_to_db(employee_num, prediction, raw_data, feature_df)\n",
        "    \n",
        "    if result:\n",
        "        print(\"\\n✅ Prediction pipeline completed successfully!\")\n",
        "        print(f\"   Prediction stored in MongoDB at: /mongo/predictions/\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  Prediction made but could not be logged to database\")\n",
        "else:\n",
        "    print(\"\\n❌ Prediction pipeline incomplete - missing data or model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook:\n",
        "1. ✅ Fetches the latest employee entry from the API (by `created_at` timestamp, not employee_number)\n",
        "2. ✅ Handles missing data with default values\n",
        "3. ✅ Preprocesses input data (encodes categorical variables)\n",
        "4. ✅ Loads the trained linear regression model\n",
        "5. ✅ Makes predictions on the employee data\n",
        "6. ✅ Logs results to MongoDB via API endpoint\n",
        "\n",
        "The prediction results are stored in the `predictions` collection in MongoDB via the `/mongo/predictions/` endpoint with:\n",
        "- Employee number (no foreign key constraint)\n",
        "- Predicted monthly income\n",
        "- Prediction timestamp\n",
        "- Input features used (as JSON)\n",
        "- Model version\n",
        "\n",
        "**No foreign key relations** - MongoDB stores employee_number as a simple integer field.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
